{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SkipTest",
     "evalue": "You are importing theano.sandbox.cuda. This is the old GPU back-end and is removed from Theano. Use Theano 0.9 to use it. Even better, transition to the new GPU back-end! See https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSkipTest\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-38532347d35a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnesterov_momentum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnolearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlasagne\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNeuralNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonlinearities\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonlinearities\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/nolearn/lasagne/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from .handlers import (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mPrintLayerInfo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mPrintLog\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mRememberBestWeights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mSaveWeights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/nolearn/lasagne/handlers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mansi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_conv_infos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_conv2d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/nolearn/lasagne/util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmaxpoollayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mMaxPool2DLayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda_convnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv2DCCLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda_convnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMaxPool2DCCLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mconvlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2DCCLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/lasagne/layers/cuda_convnet.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m                       \"up to Theano 0.9).\")  # pragma: no cover\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msandbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgpu_contiguous\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpylearn2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msandbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda_convnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_acts\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFilterActs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/sandbox/cuda/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# the folder will be skipped by nosetests without failing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m raise SkipTest(\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;34m\"You are importing theano.sandbox.cuda. This is the old GPU back-end and \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;34m\"is removed from Theano. Use Theano 0.9 to use it. Even better, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m\"transition to the new GPU back-end! See \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSkipTest\u001b[0m: You are importing theano.sandbox.cuda. This is the old GPU back-end and is removed from Theano. Use Theano 0.9 to use it. Even better, transition to the new GPU back-end! See https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "from lasagne import layers\n",
    "from lasagne.updates import nesterov_momentum\n",
    "from lasagne.updates import adam\n",
    "from nolearn.lasagne import NeuralNet\n",
    "from lasagne.nonlinearities import softmax\n",
    "from lasagne.nonlinearities import sigmoid\n",
    "\n",
    "\n",
    "from lasagne.layers import get_output, InputLayer, DenseLayer, Upscale1DLayer, ReshapeLayer, DropoutLayer\n",
    "from lasagne.nonlinearities import rectify, leaky_rectify, tanh\n",
    "from lasagne.updates import nesterov_momentum\n",
    "from lasagne.objectives import categorical_crossentropy\n",
    "from nolearn.lasagne import NeuralNet, BatchIterator, PrintLayerInfo\n",
    "\n",
    "from lasagne.layers import Conv1DLayer as Conv1DLayerSlow\n",
    "from lasagne.layers import MaxPool1DLayer as MaxPool1DLayerSlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    from lasagne.layers.cuda_convnet import Conv1DCCLayer \n",
    "    from lasagne.layers.cuda_convnet import MaxPool1DCCLayer \n",
    "    print('Using cuda_convnet (faster)')\n",
    "except:\n",
    "    from lasagne.layers import Conv1DLayer\n",
    "    from lasagne.layers import MaxPool1DLayer\n",
    "    print('Using lasagne.layers (slower)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.read_csv(\"./data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__type = \"math\"#sys.argv[1]\n",
    "t[__type] =  t[\"labels\"].str.contains(__type)\n",
    "t[__type] = pd.to_numeric(t[__type], downcast=\"signed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = \"problem-statement\"\n",
    "output = t[[__type,feature]]\n",
    "output = output.dropna()\n",
    "\n",
    "output[__type] = output[__type].astype(int)\n",
    "\n",
    "df = pd.DataFrame(columns=[__type, feature])\n",
    "\n",
    "a = output.loc[output[__type] == 1.0]\n",
    "b = output.loc[output[__type] == 0.0]\n",
    "b = b.sample(n=len(a), random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = a.append(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenize, stop_words=\"english\",analyzer = 'word')\n",
    "\n",
    "l = output[feature].tolist()\n",
    "res = []\n",
    "regex = re.compile('^a-zA-Z0-9')\n",
    "def clean_s(input_str):\n",
    "    return regex.sub(' ', input_str)\n",
    "\n",
    "for i in range(len(l)):\n",
    "    try:\n",
    "        res.append(clean_s(l[i]))\n",
    "    except:\n",
    "        res.append(\"\")\n",
    "\n",
    "X = vectorizer.fit_transform(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = linear_model.RidgeCV(alphas = [0.1, 1.0, 10])\n",
    "#parameters = {}\n",
    "#model = linear_model.Lasso(alpha=0.0001)\n",
    "#model = linear_model.SGDClassifier(loss = 'hinge', penalty='l2')\n",
    "#parameters = {}\n",
    "#model = GaussianNB()\n",
    "\n",
    "### -->logistic regression\n",
    "#model = linear_model.LogisticRegression(class_weight= 'balanced')\n",
    "#parameters = {'solver':('newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'), 'C':[0.1, 1, 10,100], 'max_iter': [100,1000]}\n",
    "\n",
    "### -> logistic regression\n",
    "#model = LinearSVC()\n",
    "#model = MLPClassifier(solver='lbfgs',  hidden_layer_sizes= (100,100,100), random_state=1)\n",
    "#parameters={}\n",
    "#parameters = {'C':[ 0.1, 1, 10,100]}\n",
    "### --> randomforest\n",
    "#model = RandomForestClassifier(class_weight = 'balanced')\n",
    "#parameters={}\n",
    "### --> random forest\n",
    "\n",
    "\n",
    "#model = AdaBoostClassifier(n_estimators=100)\n",
    "#parameters= {'n_estimators': (10,100,1000)}\n",
    "#### ---> SVC\n",
    "#model = svm.SVC()\n",
    "#parameters = {'kernel':('linear', 'rbf', 'poly', 'sigmoid'), 'C':[ 1, 10]}\n",
    "### ---> SVC \n",
    "#clf = GridSearchCV(model, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,output[__type], test_size=0.2,random_state=42)\n",
    "print X_train.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_in = 'full'\n",
    "pad_out = 'full'\n",
    "layers0 = [\n",
    "    # layer dealing with the input data\n",
    "    (InputLayer, {'shape': (None, 1, X_train.shape[1] )}),\n",
    "\n",
    "    # first stage of our convolutional layers\n",
    "    (Conv1DLayer, {'num_filters': 16, 'filter_size': 4, 'pad':'full'}),\n",
    "    (Conv1DLayer, {'num_filters': 16, 'filter_size': 4, 'pad':'full'}),\n",
    "    (Conv1DLayer, {'num_filters': 16, 'filter_size': 4, 'pad':'full'}),\n",
    "    (Conv1DLayer, {'num_filters': 16, 'filter_size': 4, 'pad':'full'}),\n",
    "    (Conv1DLayer, {'num_filters': 16, 'filter_size': 4, 'pad':'full'}),\n",
    "    (MaxPool1DLayer, {'pool_size': 2}),\n",
    "\n",
    "    # second stage of our convolutional layers\n",
    "    (Conv1DLayer, {'num_filters': 28, 'filter_size': 4, 'pad':'full'}),\n",
    "    (Conv1DLayer, {'num_filters': 28, 'filter_size': 4, 'pad':'full'}),\n",
    "    (Conv1DLayer, {'num_filters': 28, 'filter_size': 4, 'pad':'full'}),\n",
    "    (MaxPool1DLayer, {'pool_size': 2}),\n",
    "\n",
    "    # two dense layers with dropout\n",
    "    (DenseLayer, {'num_units': 64}),\n",
    "    (DropoutLayer, {}),\n",
    "    (DenseLayer, {'num_units': 64}),\n",
    "\n",
    "    # the output layer\n",
    "    (DenseLayer, {'num_units': 1, 'nonlinearity': sigmoid}),\n",
    "]\n",
    "ae = NeuralNet(\n",
    "    layers=layers0,\n",
    "    max_epochs=20,\n",
    "    \n",
    "    update=nesterov_momentum,\n",
    "    update_learning_rate=0.01,\n",
    "    update_momentum=0.975,\n",
    "    \n",
    "    regression=True,\n",
    "    verbose=1\n",
    ")\n",
    "x=X_train.toarray().reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "print X_train.shape\n",
    "print y_train.shape\n",
    "ae.fit(x, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for neural network only\n",
    "net1 = NeuralNet(\n",
    "    layers=[  # three layers: one hidden layer\n",
    "        ('input', layers.InputLayer),\n",
    "        ('hidden1', layers.DenseLayer),\n",
    "        ('hidden2', layers.DenseLayer),\n",
    "        ('hidden3', layers.DenseLayer),\n",
    "        ('hidden4', layers.DenseLayer),\n",
    "        ('hidden5', layers.DenseLayer),\n",
    "        #('hidden6', layers.DenseLayer),\n",
    "        #('hidden7', layers.DenseLayer),\n",
    "        #('hidden8', layers.DenseLayer),\n",
    "        #('hidden9', layers.DenseLayer),\n",
    "        #('hidden10', layers.DenseLayer),\n",
    "        ('output', layers.DenseLayer),\n",
    "        ],\n",
    "    # layer parameters:\n",
    "    input_shape=(None, X_train.shape[1]), \n",
    "    hidden1_num_units=1000,  # number of units in hidden layer\n",
    "    hidden2_num_units=1000,\n",
    "    hidden3_num_units=1000,\n",
    "    hidden4_num_units=1000,\n",
    "    hidden5_num_units=1000,\n",
    "   # hidden6_num_units=100,\n",
    "   # hidden7_num_units=100,\n",
    "   # hidden8_num_units=100,\n",
    "   # hidden9_num_units=100,\n",
    "   # hidden10_num_units=100,\n",
    "    output_nonlinearity=sigmoid,  # output layer uses identity function\n",
    "    output_num_units=1,  # 1 target values\n",
    "\n",
    "    # optimization method:\n",
    "    update=adam,\n",
    "    update_learning_rate=0.001,\n",
    "    #update_momentum=0.9,\n",
    "    #classification = True,\n",
    "    regression=True,  # flag to indicate we're dealing with regression problem\n",
    "    max_epochs=200,  # we want to train this many epochs\n",
    "    verbose=1,\n",
    "    )\n",
    "net1.fit(X_train.toarray(), y_train) \n",
    "        \n",
    "# ------>end \n",
    "\n",
    "#clf.fit(X_train, y_train)\n",
    "#print sorted(clf.cv_results_.keys())\n",
    "\n",
    "y_pred = net1.predict(X_test.toarray())\n",
    "y_pred = [round(value) for value in y_pred]\n",
    "\n",
    "print __type\n",
    "print classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
